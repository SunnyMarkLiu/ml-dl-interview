# Decision Trees
## 1. RF、GBDT、XGBoost的原理和优缺点
### RF
提到随机森林，就不得不提Bagging，Bagging可以简单的理解为：** 放回抽样，多数表决（分类）或简单平均（回归）, 同时Bagging的基学习器之间属于并列生成，不存在强依赖关系。 **

Random Forest（随机森林）是Bagging的扩展变体，它在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了 **随机特征选择**，因此可以概括RF包括四个部分：**1、随机选择样本（放回抽样）；2、随机选择特征；3、构建决策树；4、随机森林投票（平均）。**

随机选择样本和Bagging相同，随机选择特征是指在树的构建中，会从样本集的特征集合中随机选择部分特征，然后再从这个子集中选择最优的属性用于划分，这种随机性导致随机森林的偏差会有稍微的增加（相比于单棵不随机树），因为特征的随机筛选丢失了其他特征的信息，但是由于 **随机森林的‘平均’特性，会使得它的方差减小，而且方差的减小补偿了偏差的增大，因此总体而言是更好的模型。**

在构建决策树的时候，**RF的每棵决策树都最大可能的进行生长而不进行剪枝**；在对预测输出进行结合时，RF通常对分类问题使用简单投票法，回归任务使用简单平均法。

RF的重要特性是不用对其进行交叉验证或者使用一个独立的测试集获得无偏估计，**它可以在内部进行评估**，也就是说在生成的过程中可以对误差进行无偏估计，由于每个基学习器只使用了训练集中约63.2%的样本，剩下约36.8%的样本可用做验证集来对其泛化性能进行“包外估计”。

RF和Bagging对比：RF的起始性能较差，特别当只有一个基学习器时，随着学习器数目增多，随机森林通常会收敛到更低的泛化误差。随机森林的训练效率也会高于Bagging，因为在单个决策树的构建中，**Bagging 在选择特征划分结点时，要对所有的特征进行考虑，而随机森林使用的是‘随机性’特征数，只需考虑特征的子集。**

### GBDT
提GBDT之前，谈一下Boosting，Boosting是一种与Bagging很类似的技术。不论是Boosting还是Bagging，所使用的多个分类器类型都是一致的。但是在Boosting当中，不同的分类器是通过串行训练而获得的，每个新分类器都根据已训练的分类器的性能来进行训练。**Boosting是通过关注被已有分类器错分的那些数据来获得新的分类器。**
由于Boosting分类的结果是基于所有分类器的 **加权求和** 结果的，因此Boosting与Bagging不太一样，**Bagging中的分类器权值是一样的，而Boosting中的分类器权重并不相等，每个权重代表对应的分类器在上一轮迭代中的成功度。**

GBDT与传统的Boosting区别较大，它的每一次计算都是为了减少上一次的残差，而为了消除残差，我们可以 **在残差减小的梯度方向上建立模型,所以说在GradientBoost中，每个新的模型的建立是为了使得之前的模型的残差往梯度下降的方法，与传统的Boosting中关注正确错误的样本加权有着很大的区别。**

在GradientBoosting算法中，关键就是利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一棵CART回归树。

**GBDT的会累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT的树都是CART回归树，而不是分类树。**（尽管GBDT调整后也可以用于分类但不代表GBDT的树为分类树）。

### XGBoost

## 2. RF、GBDT、Boost、Xgboost 的损失函数是什么？
https://www.zybuluo.com/yxd/note/611571

## 1. GBDT和XGBoost的区别是什么？
https://www.julyedu.com/question/big/kp_id/23/ques_id/987

## 2. 为什么xgboost要用泰勒展开，优势在哪里？
https://www.julyedu.com/question/big/kp_id/23/ques_id/990

## 3. xgboost如何寻找最优特征？是有放回还是无放回的呢？
https://www.julyedu.com/question/big/kp_id/23/ques_id/992
