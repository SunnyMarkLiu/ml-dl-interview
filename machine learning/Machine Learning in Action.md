# 机器学习实践总结
## 1. 一个完整的机器学习项目流程
（1）数学抽象
- 明确问题是进行机器学习的第一步。机器学习的训练过程通常都是一件非常耗时的事情，胡乱尝试时间成本是非常高的。
- 这里的抽象成数学问题，指的是根据数据明确任务目标，是分类、还是回归，或者是聚类。

（2）数据获取
- 数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。
- 数据要有代表性，否则必然会过拟合。
- 对于分类问题，数据偏斜不能过于严重（平衡），不同类别的数据数量不要有数个数量级的差距。
- 对数据的量级要有一个评估，多少个样本，多少个特征，据此估算出内存需求。如果放不下就得考虑改进算法或者使用一些降维技巧，或者采用分布式计算。

（3）预处理与特征选择
- 良好的数据要能够提取出良好的特征才能真正发挥效力。
- 预处理/数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。
- 筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。

（4）模型训练与调优
- 直到这一步才用到我们上面说的算法进行训练。
- 现在很多算法都能够封装成黑盒使用。但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。这需要我们对算法的原理有深入的理解。理解越深入，就越能发现问题的症结，提出良好的调优方案。

（5）模型诊断
- 如何确定模型调优的方向与思路呢？这就需要对模型进行诊断的技术。
- 过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。
- 误差分析也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题......
- 诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。

（6）模型融合/集成
- 一般来说，模型融合后都能使得效果有一定提升。而且效果很好。
- 工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。

（7）上线运行
- 这一部分内容主要跟工程实现的相关性更大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。
- 这些工作流程主要是工程实践上总结出的一些经验。并不是每个项目都包含完整的一个流程。这里的部分只是一个指导性的说明，只有多实践，多积累项目经验，才会有自己更深刻的认识。

## 2. 数据清洗与特征处理的常用方法？
![](../assets/machine_learning/feature_engineer.jpg)

## 3. 常见异常点检测方法有哪些？
### 异常点检测算法使用场景
常见的有三种情况：
- 在做特征工程的时候需要对异常的数据做过滤，防止对归一化等处理的结果产生影响；
- 对没有标记输出的特征数据做筛选，找出异常的数据，即无监督学习任务中；
- 对有标记输出的特征数据做二分类时，由于某些类别的训练样本非常少，类别严重不平衡，此时也可以考虑用非监督的异常点检测算法来做。

### 异常点检测常用的方法
- 采用 EDA+统计的方式，通过绘图直接观察数据点的分布，如绘制散点图或箱形图，观察分位数的分布情况；
- 基于聚类的方法来做异常点检测，由于大部分聚类算法是基于数据特征的分布来做的，通常如果我们聚类后发现某些聚类簇的数据样本量比其他簇少很多，而且这个簇里数据的特征均值分布之类的值和其他簇也差异很大，这些簇里的样本点大部分时候都是异常点。
- 基于专门的异常点检测算法，如 One Class SVM 和 Isolation Forest.

One Class SVM

期望所有不是异常的样本都是正类别，同时它采用一个超球体而不是一个超平面来做划分，该算法在特征空间中获得数据周围的球形边界，期望最小化这个超球体的体积，从而最小化异常点数据的影响。

假设产生的超球体参数为中心 o 和对应的超球体半径 r>0 ，超球体体积 V(r) 被最小化，中心 o 是支持向量的线性组合；跟传统SVM方法相似，可以要求所有训练数据点 xi 到中心的距离严格小于 r，但同时构造一个惩罚系数为 C 的松弛变量 ξi。
